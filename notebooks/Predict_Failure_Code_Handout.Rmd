---
title: "Code_Handout"
author: "Jackie Vogel, Sarah Millard, Ignacio Luque, Leo Walker"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(nnet) # AKA Fit Neural Networks. If the response in formula is a factor, an appropriate classification network is constructed
library(caret) # has several functions that attempt to streamline the model building and evaluation process (train/test split)
library(ggplot2) # Helps with elegant data visualization
library(car) # Usage of functions inside user functions
library(knitr) # A general-purpose tool for dynamic report generation
library (dplyr) # Helps with data manipulation
library(GGally) # An extension of GGPLOT2 - visualization of plots
library(patchwork) # Helps with composition of plots by, among others, providing mathematical operators for combining multiple plots
library(corrplot) # Helps with the visualization of a correlation matrix
library(DataExplorer) # Helps with data exploration process for data analysis and model building
```


### Multinomial Logistic Regression

This R Markdown demonstrates how to use Multinomial Logistic Regression on a small dataset that is predicting the type of machine failure based on several different variables.

Dataset is from: https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification

It is a synthetic dataset that reflects what is encountered in industry. 

```{r}
pred_maint <-read.csv("predictive_maintenance.csv", header=TRUE, sep= ",")
str(pred_maint) # Display of data

table(pred_maint$Target) # Target 0 or 1 - Failure or No Failure in the machines

table(pred_maint$Failure_Type) # Count of the different types of failures
```

#### Independent Variables

**UID**: unique identifier ranging from 1 to 10000

**ProductID**: consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number

**Type**: consisting of the letter L, M or H from the productID.

**Air temperature [K]**: generated using a random walk process later normalized to a standard deviation of 2 K around 300 K

**Process temperature [K]**: generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.

**Rotational speed [rpm]**: calculated from power of 2860 W, overlaid with a normally distributed noise

**Torque [Nm]**: torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values.

**Tool wear [min]**: The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process. and a
'machine failure' label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true.


#### Dependent Variables

**Target**: A target variable that indicates that a failure occurred. 0 indicates No Failure, 1 indicates some type of Failure.

**Failure_Type**: The type of failure that occurred. The types are: No Failure and Random Failures which occur when target is 0; Power Failure, Tool Wear Failure, Overstrain Failure, and Heat Dissipation Failure which occur when target is 1.

#### Research Question

Using Multinomial Logistic Regression on this data set, we can predict what type of machine failure will occur based on the type of wear it has seen. 


### Dataset preparation

- Limit the dataset to where "target" == 1 so that we only have occurrences of failures
- Remove unnecessary variables like UID, Product_ID, and Target
- Change remaining chr types to factors

```{r}
# limit the predict failure data frame to only when failures occurred
pred_failure <- pred_maint[pred_maint$Target ==1,]
pred_failure <- pred_failure[pred_failure$Failure_Type != "No Failure",]
pred_failure <- pred_failure[,!names(pred_failure) %in% c("UDI", "Product_ID", "Target")]

# set the chr types to factors
pred_failure$Type <- as.factor(pred_failure$Type)
pred_failure$Failure_Type <- as.factor(pred_failure$Failure_Type)

pred_failure <- pred_failure |> 
  mutate(obs_num = 1:n())

str(pred_failure)
```

## Check that the independent variables are linear with the predicted variable

A test that can be used to check linearity is the Box-Tidewell test. This test checks the logit the predictor variable against a log transformation of the independent variables. Because its a log transformation only continuous independent variables with non-negative values can be tested.

```{r}
lreg <- glm(Failure_Type ~ Air_temperature_K + Process_temperature_K + Rotational_speed_rpm + Torque_Nm ,data=pred_failure, family = binomial(link="logit"))

#getting the logit of FailureType
logodds <- lreg$linear.predictors

boxTidwell(logodds ~ Air_temperature_K + Process_temperature_K + Rotational_speed_rpm + Torque_Nm ,other.x = ~Type ,data = pred_failure)

```
The Box-Tidewell test has a null hypotheses that the logit of Failure Type and the log transformation of the independent variable is linear. 

Therefore when interpreting the table and using the p-value of .05 we see that We do NOT reject the null for variables Air_temperature_K,Process_temperature_K , Rotational_speed_rpm and we can conclude that there is not enough statistical evidence to reject the null hypotheses that the relationship is linear.

However for Torque_Nm. We see that the p-value is >.05 and therefore we reject the null hypotheses and conclude that there is enough statistical evidence to reject the null hypotheses that the relationship is linear.

Since MLR requires the relationship to be linear we throw out Torque_NM from our model. 

A visual representation of the linearity assumption would be as follows:

```{r}

model <-
  multinom(
    Failure_Type ~ Air_temperature_K + Process_temperature_K + Rotational_speed_rpm + Tool.wear_min,
    data = pred_failure
  )

summary(model)

# Residuals
# Calculate residuals
residuals <- as_tibble(residuals(model)) %>% #calculate residu
  setNames(paste('resid.', names(.), sep = "")) %>% #update column
  mutate(obs_num = 1:n()) #add obs number

# Calculate predicted probabilities (fitted values)
pred_probs <- as_tibble(predict(model, type = "probs")) %>%
  mutate(obs_num = 1:n())


pred_failure_1 <- inner_join(pred_failure, pred_probs) #add probs
pred_failure_1 <- inner_join(pred_failure_1, residuals) #add resid

residual_plot <- function(data, x, y, title) {
  data |> 
    ggplot(aes(x = x, y = y)) +
    geom_point(size = 2) +
    geom_hline(yintercept = 0, lty = 2, size = 1, colour = "red") +
    labs(x = "Predicted", y = "Residuals", title = paste(title)) +
    theme_test() +
    theme(plot.title = element_text(hjust = 0.5))
}

```

### Residual vs Air Temperature

Once the fitted values and residuals are found then you would plot the residuals vs fitted values for all the levels in your target variable and for every variable in your model. Example is for only one of the variables 

```{r}
p1 <-
  residual_plot(
    pred_failure_1,
    pred_failure_1$`Air_temperature_K`,
    pred_failure_1$`resid.Heat Dissipation Failure`,
    title = "Heat Dissipation Failure") +
  labs(x = "Air_temperature_K")
p2 <-
  residual_plot(
    pred_failure_1,
    pred_failure_1$`Air_temperature_K`,
    pred_failure_1$`resid.Overstrain Failure`,
    title = "Overstrain Failure") +
  labs(x = "Air_temperature_K")
p3 <-
  residual_plot(
    pred_failure_1, 
    pred_failure_1$`Air_temperature_K`,
    pred_failure_1$`resid.Power Failure`, 
    title = "Power Failure") +
  labs(x = "Air_temperature_K")
p4 <-
  residual_plot(
    pred_failure_1,
    pred_failure_1$`Air_temperature_K`,
    pred_failure_1$`resid.Tool Wear Failure`,
    title = "Tool Wear Failure") +
  labs(x = "Air_temperature_K")


p1 + p2 + p3 + p4 + plot_layout(ncol = 2)
```

## Ensure that each observation is independent from the others and the outcome categories are mutually exclusive. 

We can intuitively confirm that each observation is independent from the others because there isn't any interaction between each machine. We also know that the outcome categories are mutually exclusive because there can only be one reason for the failure type if any.


## Check for multicollinearity between variables

```{r}

# Visual representation of mulitcollinearty 
num_vars <- c("Air_temperature_K", "Process_temperature_K", "Rotational_speed_rpm", "Tool.wear_min")
M <- cor(pred_failure[num_vars])
corrplot(M, method="circle")


# Using Variance Inflation Factor
VIF.model<-glm(Failure_Type ~ Air_temperature_K + Process_temperature_K + Rotational_speed_rpm + Tool.wear_min,family = "binomial",data = pred_failure)

vif<-vif(VIF.model)

vif

# Correlation graph
pred_failure |> 
  select(num_vars) |> 
  ggpairs()

```

Looking at the visual representation we see  two pairs of variables that are highly correlated with each other, so we will have to remove one of each respectively.:

- Process_temperature_K is correlated with Air_temperature_K 

Using Variance Inflation Factor the rule of thumb is that 
-A value of 1 indicates no correlation between predictor variable  
-A value between 1 and 5 indicates moderate correlations between predictor variable 
-A value greater than 5 indicates severe correlation between predictor variable 

Again we see that Process_temperature_K is moderately correlated with Air_temperature_K 

After running iterations with different pairs it looks like removing Air_temperature_K  gives us our best accuracy. 


## Cleaning up dataset 

```{r}
pred_failure_lim <- pred_failure[c("Type", "Process_temperature_K", "Rotational_speed_rpm", "Tool.wear_min", "Failure_Type")]
str(pred_failure_lim)
```

## Data preparation:

Making Process_temperature_K a standard scale

```{r}
# scaling the process_temperature_K to put it in a more standard range
pred_failure_lim$Process_temperature_K <- scale(pred_failure_lim$Process_temperature_K)
```


## Splitting the data in train and test

```{r}
set.seed(424242) # setting the seed to always get the same train/test split
#Splitting the data using a function from dplyr package
index <- createDataPartition(pred_failure_lim$Failure_Type, p = .80, list = FALSE)
train <- pred_failure_lim[index,]
test <- pred_failure_lim[-index,]
```


## Creating a mulitnomial Logiestice Regression model using the training data 

```{r}
set.seed(424242) # setting the seed to always get the same model_fit

# Training the multinomial model
multinom_model <- multinom(Failure_Type ~ ., data = train)

# Checking the model
summary(multinom_model)
```
In this output, Heat Dissipation Failure is the dependent variable that we are using to compare all the other variables. How these results are interpreted differs depending on whether the independent variable is continuous or categorical.

Looking at the variables Process_temperature_K and Power Failure. The results would be interpreted as follows:

A one-unit increase in the variable Process_temperature_K is associated with the decrease in the log odds of being a Heat Dissipation Failure vs. Power Failure by 0.829.

Looking at the variables TypeL and Tool Wear Failure. The results would be interpreted as follows:

The log odds of being a Heat Dissipation Failure vs. Tool Wear Failure. will decrease by 0.792 if moving from Type high to Type low.


### Predicting Failure Type on Train Dataset and Creating Confusion matrix

```{r}
# Predicting the values for train dataset
train$Failure_Predicted <- predict(multinom_model, newdata = train, "class")

# Building classification table
tab <- table(train$Failure_Type, train$Failure_Predicted)
cm <- confusionMatrix(train$Failure_Predicted, train$Failure_Type)
cm_class <- cm$byClass
cm
recall <- mean(cm_class[,"Sensitivity"])
recall
precision <- mean(cm_class[,"Pos Pred Value"])
precision
```
It looks like with the train set that we have 73% accuracy, 73% recall (Sensitivity) and 75% precision (Pos Pred Value)


### Predicting Failure Type on Test Dataset

This is answering our question can predict what type of machine failure will occur based on the type of wear it has seen. 

```{r}
# Predicting the class for test dataset
test$Failure_Predicted <- predict(multinom_model, newdata = test, "class")

# Building classification table
cm <-confusionMatrix(test$Failure_Predicted, test$Failure_Type)
cm_class <- cm$byClass
cm
recall <- mean(cm_class[,"Sensitivity"])
recall
precision <- mean(cm_class[,"Pos Pred Value"])
precision
```
It looks like with the test set that we have 78% accuracy, 77% recall (Sensitivity) and 82% precision (Pos Pred Value)

Meaning, we are 78% accurate on the prediction what type of machine failure will occur based on the type of wear it has seen on the machine thanks to test data and the interpretation made on the data provided on the multinomial logistic regression . 


###The end.
